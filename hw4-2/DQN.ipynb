{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "EPOCHS_NUM = 4000000\n",
    "BUFFER_SIZE = 10000000\n",
    "GAMMA = 0.99\n",
    "INITIAL_EPISLON = 1.0\n",
    "FINAL_EPISLON = 0.1\n",
    "FINAL_EXPLORATION_FRAME = 10000000\n",
    "REPLAY_START_SIZE = 50000\n",
    "\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img)).reshape(105, 80, 1)\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Define the model from online\n",
    "import keras\n",
    "from keras.layers import Input, Lambda, Convolution2D, Flatten, Dense, Multiply\n",
    "from keras.models import Model\n",
    "def atari_model(n_actions):\n",
    "    \n",
    "    # Channel last\n",
    "    ATARI_SHAPE = (105, 80, 4)\n",
    "\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "    \n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = keras.layers.Convolution2D(\n",
    "        16, 8, strides=(4, 4), activation='relu'\n",
    "    )(normalized)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = keras.layers.Convolution2D(\n",
    "        32, 4, strides=(2, 2), activation='relu'\n",
    "    )(conv_1)\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = keras.layers.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = keras.layers.Dense(n_actions)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = keras.layers.Multiply() ([output, actions_input])\n",
    "\n",
    "    model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "    optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss='mse')\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "ReplayBuffer Class:\n",
    "the implementation of the replay buffer for\n",
    "storing the previous episodes information\n",
    "with 5 tuple (s, a, r, s', d)\n",
    "based on tensorflow implementaion with modification\n",
    "Planned extention:\n",
    "1. Implementation of Prioritized Experience Buffer\n",
    "'''\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    # initialize Buffer with the maximimum size\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.cur_index = 0\n",
    "        self.cur_size = 0\n",
    "        self.buffer = {}\n",
    "\n",
    "    # add the new sample into the buffer\n",
    "    def add_sample(self, sample):\n",
    "        self.buffer[self.cur_index] = sample\n",
    "        self.cur_index = (self.cur_index + 1) % self.max_size\n",
    "        self.cur_size = min(self.cur_size + 1, self.max_size)\n",
    "\n",
    "    # get the batch of according to size\n",
    "    def get_batch(self, batch_size):\n",
    "        batch_size = min(self.cur_size, batch_size)\n",
    "        idxs = random.sample(range(self.cur_size), batch_size)\n",
    "\n",
    "        result = zip(*[self.buffer[idx] for idx in idxs])\n",
    "        result = [np.array(list(r)) for r in result]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
    "    \"\"\"Do one deep Q learning iteration.\n",
    "    \n",
    "    Params:\n",
    "    - model: The DQN\n",
    "    - gamma: Discount factor (should be 0.99)\n",
    "    - start_states: numpy array of starting states\n",
    "    - actions: numpy array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: numpy array of rewards corresponding to the start states and actions\n",
    "    - next_states: numpy array of the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: numpy boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
    "    # The Q values of the terminal states is 0 by definition, so override them\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
    "    # the targets by the actions.\n",
    "    model.fit(\n",
    "        [start_states, actions], actions * Q_values[:, None],\n",
    "        nb_epoch=1, batch_size=len(start_states), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_action(model, state):\n",
    "    \n",
    "    all_actions_Q = model.predict([state.reshape(1, 105, 80, 4), np.ones((state.shape[0], 4))])\n",
    "    \n",
    "    result_action = np.argmax(all_actions_Q, axis =1)\n",
    "    \n",
    "    print(result_action[0])\n",
    "    \n",
    "    return result_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_iteration(env, model, state, iteration, memory):\n",
    "    # Choose epsilon based on the iteration\n",
    "    epsilon = get_epsilon_for_iteration(iteration)\n",
    "\n",
    "    # Choose the action \n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = choose_best_action(model, state)\n",
    "\n",
    "    # Play one game iteration (note: according to the next paper, you should actually play 4 times here)\n",
    "    next_frame, reward, is_done, _ = env.step(action)\n",
    "    new_frame = preprocess(next_frame)\n",
    "    action_arr = np.zeros(4)\n",
    "    action_arr[action] = 1\n",
    "    action = action_arr\n",
    "    clipped_reward = transform_reward(reward)\n",
    "    saving_frame = np.copy(state[:,:,1:])\n",
    "#     print(saving_frame.shape, new_frame.shape)\n",
    "    saving_frame = np.concatenate([saving_frame, new_frame], axis = 2)\n",
    "    print(saving_frame.shape)\n",
    "    \n",
    "    memory.add_sample((state, action, clipped_reward, saving_frame, is_done))\n",
    "\n",
    "    # Sample and fit\n",
    "    start_states, actions, rewards, next_states, is_terminal = memory.get_batch(32)\n",
    "    fit_batch(model, gamma= GAMMA, start_states = start_states, actions = actions,\n",
    "              rewards = rewards, next_states = next_states, is_terminal = is_terminal)\n",
    "    \n",
    "    return reward, saving_frame, is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_epsilon_for_iteration(iteration, replay_start_size = REPLAY_START_SIZE,\n",
    "                              final_exploration_frame = FINAL_EXPLORATION_FRAME,\n",
    "                              initial_epislon = INITIAL_EPISLON, final_epislon = FINAL_EPISLON):\n",
    "    \n",
    "    # the number of iteractions\n",
    "    if iteration < replay_start_size:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return max(final_epislon, initial_epislon - float(iteration - replay_start_size) / (final_exploration_frame - replay_start_size) * (initial_epislon - final_epislon)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial of environment\n",
    "# # Import the gym module\n",
    "# import gym\n",
    "\n",
    "# # Create a breakout environment\n",
    "# env = gym.make('BreakoutDeterministic-v4')\n",
    "# # Reset it, returns the starting frame\n",
    "# frame = env.reset()\n",
    "# # Render\n",
    "# env.render()\n",
    "\n",
    "# is_done = False\n",
    "# while not is_done:\n",
    "#   # Perform a random action, returns the new frame, reward and whether the game is over\n",
    "#   frame, reward, is_done, _ = env.step(env.action_space.sample())\n",
    "#   # Render\n",
    "#   env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mu...)`\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "frames (InputLayer)             (None, 105, 80, 4)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 105, 80, 4)   0           frames[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 25, 19, 16)   4112        lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 11, 8, 32)    8224        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 2816)         0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 256)          721152      flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 4)            1028        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 4)            0           dense_12[0][0]                   \n",
      "                                                                 mask[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 734,516\n",
      "Trainable params: 734,516\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n",
      "(105, 80, 4)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-85d069f82c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#         print(frame.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         reward, frame, is_done = q_iteration(env = env, model = model,\n\u001b[0;32m---> 26\u001b[0;31m                     state = frame, iteration = itr, memory = memory)\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mitr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-5da78f56c7ca>\u001b[0m in \u001b[0;36mq_iteration\u001b[0;34m(env, model, state, iteration, memory)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mstart_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     fit_batch(model, gamma= GAMMA, start_states = start_states, actions = actions,\n\u001b[0;32m---> 28\u001b[0;31m               rewards = rewards, next_states = next_states, is_terminal = is_terminal)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fecd23bbdbb5>\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(model, gamma, start_states, actions, rewards, next_states, is_terminal)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# First, predict the Q values of the next states. Note how we are passing ones as the mask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mnext_Q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# The Q values of the terminal states is 0 by definition, so override them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnext_Q_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "\n",
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "# Reset it, returns the starting frame\n",
    "\n",
    "\n",
    "memory = ReplayBuffer(BUFFER_SIZE)\n",
    "model = atari_model(env.action_space.n)\n",
    "itr = 0\n",
    "for i in range(EPOCHS_NUM):\n",
    "    \n",
    "    \n",
    "    frame = env.reset()\n",
    "    frame = preprocess(frame)\n",
    "    frame = [np.copy(frame) for _ in range(4)]\n",
    "    frame = np.concatenate(frame, axis = 2)\n",
    "    is_done = False\n",
    "    total_rewards = 0\n",
    "    while not is_done:\n",
    "        \n",
    "#         print(frame.shape)\n",
    "        reward, frame, is_done = q_iteration(env = env, model = model,\n",
    "                    state = frame, iteration = itr, memory = memory)\n",
    "        total_rewards += reward\n",
    "        itr += 1\n",
    "    \n",
    "    print('ep %d : %lf' %(i, total_rewards))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
